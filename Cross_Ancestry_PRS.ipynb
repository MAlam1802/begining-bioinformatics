{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOxzGbwIUCHPFEfUEzYgXYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAlam1802/begining-bioinformatics/blob/main/Cross_Ancestry_PRS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PwRhJ-UNdU-A"
      },
      "outputs": [],
      "source": [
        "# Core scientific + plotting stack\n",
        "!pip install -q \\\n",
        "    pandas \\\n",
        "    numpy \\\n",
        "    scipy \\\n",
        "    scikit-learn \\\n",
        "    statsmodels \\\n",
        "    matplotlib \\\n",
        "    seaborn \\\n",
        "    tqdm \\\n",
        "    pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plotting defaults\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
        "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
        "\n",
        "print(\"Libraries imported and ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiq5wxVw14R",
        "outputId": "c073e722-32cb-4921-f2dd-56f3fbe60f3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install plink 1.9 binary\n",
        "!wget -qO- http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20201019.zip > plink.zip\n",
        "!unzip -o plink.zip\n",
        "!chmod +x plink\n",
        "!mv plink /usr/local/bin/\n",
        "\n",
        "!plink --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgmj1_qSxKNh",
        "outputId": "7a9bc9aa-33b9-4cdc-eab4-272afb0c9222"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  plink.zip\n",
            "  inflating: plink                   \n",
            "  inflating: LICENSE                 \n",
            "  inflating: toy.ped                 \n",
            "  inflating: toy.map                 \n",
            "  inflating: prettify                \n",
            "PLINK v1.90b6.21 64-bit (19 Oct 2020)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "eur_path = \"GCST90320058EUR.h.tsv.gz\"\n",
        "afr_path = \"GCST90320061AFR.h.tsv.gz\"\n",
        "\n",
        "# Loading with pandas\n",
        "eur_df = pd.read_csv(eur_path, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n",
        "afr_df = pd.read_csv(afr_path, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n",
        "\n",
        "print(\"Loaded EUR GWAS:\", eur_df.shape)\n",
        "print(\"Loaded AFR GWAS:\", afr_df.shape)\n",
        "print(\"EUR columns:\", list(eur_df.columns))\n",
        "print(\"AFR columns:\", list(afr_df.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWU3ZleC3K_l",
        "outputId": "5c313757-0d09-4365-a327-99d5504bc4b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded EUR GWAS: (13264860, 17)\n",
            "Loaded AFR GWAS: (19077597, 17)\n",
            "EUR columns: ['chromosome', 'base_pair_location', 'effect_allele', 'other_allele', 'beta', 'standard_error', 'effect_allele_frequency', 'p_value', 'variant_id', 'rsid', 'het_i2', 'het_p_value', 'n_samples', 'n_cases', 'n_studies', 'hm_coordinate_conversion', 'hm_code']\n",
            "AFR columns: ['chromosome', 'base_pair_location', 'effect_allele', 'other_allele', 'beta', 'standard_error', 'effect_allele_frequency', 'p_value', 'variant_id', 'rsid', 'het_i2', 'het_p_value', 'n_samples', 'n_cases', 'n_studies', 'hm_coordinate_conversion', 'hm_code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def clean_gwas(df, label=\"GWAS\"):\n",
        "    df = df.copy()\n",
        "    print(f\"\\n=== Cleaning {label} ===\")\n",
        "    print(\"Original columns:\", list(df.columns))\n",
        "\n",
        "    # --- 1. Chromosome and position ---\n",
        "    df[\"CHR\"] = pd.to_numeric(df[\"chromosome\"], errors=\"coerce\")\n",
        "    df[\"BP\"]  = pd.to_numeric(df[\"base_pair_location\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 2. SNP ID (rsid preferred) ---\n",
        "    if \"rsid\" in df.columns:\n",
        "        df[\"SNP\"] = df[\"rsid\"]\n",
        "    else:\n",
        "        df[\"SNP\"] = df[\"variant_id\"]\n",
        "\n",
        "    before = df.shape[0]\n",
        "    df = df[~df[\"SNP\"].isna()]\n",
        "    print(f\"{label}: Dropped {before - df.shape[0]} rows with missing SNP IDs.\")\n",
        "\n",
        "    # --- 3. Alleles ---\n",
        "    df[\"A1\"] = df[\"effect_allele\"].astype(str).str.upper()\n",
        "    df[\"A2\"] = df[\"other_allele\"].astype(str).str.upper()\n",
        "\n",
        "    # --- 4. Effect size, SE, p-value ---\n",
        "    df[\"BETA\"] = pd.to_numeric(df[\"beta\"], errors=\"coerce\")\n",
        "    df[\"SE\"]   = pd.to_numeric(df[\"standard_error\"], errors=\"coerce\")\n",
        "    df[\"P\"]    = pd.to_numeric(df[\"p_value\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 5. Sample size (n_samples) ---\n",
        "    df[\"N\"] = pd.to_numeric(df[\"n_samples\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 6. Keep autosomes only (1â€“22) ---\n",
        "    before = df.shape[0]\n",
        "    df = df[df[\"CHR\"].between(1, 22, inclusive=\"both\")]\n",
        "    print(f\"{label}: Dropped {before - df.shape[0]} non-autosomal rows.\")\n",
        "\n",
        "    # --- 7. Drop rows with missing critical fields ---\n",
        "    before = df.shape[0]\n",
        "    df = df.dropna(subset=[\"CHR\", \"BP\", \"SNP\", \"A1\", \"A2\", \"BETA\", \"SE\", \"P\"])\n",
        "    print(f\"{label}: Dropped {before - df.shape[0]} rows with missing fields.\")\n",
        "\n",
        "    # --- 8. Ensure numeric sanity ---\n",
        "    before = df.shape[0]\n",
        "    df = df[\n",
        "        np.isfinite(df[\"BETA\"]) &\n",
        "        np.isfinite(df[\"SE\"]) &\n",
        "        np.isfinite(df[\"P\"]) &\n",
        "        (df[\"SE\"] > 0)\n",
        "    ]\n",
        "    print(f\"{label}: Dropped {before - df.shape[0]} rows with invalid numeric values.\")\n",
        "\n",
        "    # --- 9. Final clean selection ---\n",
        "    clean = df[[\"CHR\", \"BP\", \"SNP\", \"A1\", \"A2\", \"BETA\", \"SE\", \"P\", \"N\"]].copy()\n",
        "    clean = clean.sort_values([\"CHR\", \"BP\"]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"{label}: Final clean shape: {clean.shape}\")\n",
        "    return clean\n",
        "\n",
        "# Clean both GWAS tables\n",
        "eur_clean = clean_gwas(eur_df, \"EUR_ccRCC\")\n",
        "afr_clean = clean_gwas(afr_df, \"AFR_ccRCC\")\n",
        "\n",
        "# Save full clean versions\n",
        "eur_clean_path = \"EUR_ccRCC.clean.autosomes.tsv\"\n",
        "afr_clean_path = \"AFR_ccRCC.clean.autosomes.tsv\"\n",
        "\n",
        "eur_clean.to_csv(eur_clean_path, sep=\"\\t\", index=False)\n",
        "afr_clean.to_csv(afr_clean_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"\\nSaved clean files:\")\n",
        "print(\" \", eur_clean_path)\n",
        "print(\" \", afr_clean_path)\n",
        "\n",
        "# Create minimal clumping input files\n",
        "eur_for_clump = eur_clean[[\"SNP\", \"CHR\", \"BP\", \"P\", \"A1\", \"A2\", \"BETA\"]]\n",
        "afr_for_clump = afr_clean[[\"SNP\", \"CHR\", \"BP\", \"P\", \"A1\", \"A2\", \"BETA\"]]\n",
        "\n",
        "eur_for_clump_path = \"EUR_ccRCC.for_clump.tsv\"\n",
        "afr_for_clump_path = \"AFR_ccRCC.for_clump.tsv\"\n",
        "\n",
        "eur_for_clump.to_csv(eur_for_clump_path, sep=\"\\t\", index=False)\n",
        "afr_for_clump.to_csv(afr_for_clump_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"\\nSaved clump files:\")\n",
        "print(\" \", eur_for_clump_path)\n",
        "print(\" \", afr_for_clump_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiaJRtVH3zr8",
        "outputId": "178f82da-8eb0-41e7-8f8c-452d9cf5683e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cleaning EUR_ccRCC ===\n",
            "Original columns: ['chromosome', 'base_pair_location', 'effect_allele', 'other_allele', 'beta', 'standard_error', 'effect_allele_frequency', 'p_value', 'variant_id', 'rsid', 'het_i2', 'het_p_value', 'n_samples', 'n_cases', 'n_studies', 'hm_coordinate_conversion', 'hm_code']\n",
            "EUR_ccRCC: Dropped 0 rows with missing SNP IDs.\n",
            "EUR_ccRCC: Dropped 445595 non-autosomal rows.\n",
            "EUR_ccRCC: Dropped 0 rows with missing fields.\n",
            "EUR_ccRCC: Dropped 0 rows with invalid numeric values.\n",
            "EUR_ccRCC: Final clean shape: (12819265, 9)\n",
            "\n",
            "=== Cleaning AFR_ccRCC ===\n",
            "Original columns: ['chromosome', 'base_pair_location', 'effect_allele', 'other_allele', 'beta', 'standard_error', 'effect_allele_frequency', 'p_value', 'variant_id', 'rsid', 'het_i2', 'het_p_value', 'n_samples', 'n_cases', 'n_studies', 'hm_coordinate_conversion', 'hm_code']\n",
            "AFR_ccRCC: Dropped 0 rows with missing SNP IDs.\n",
            "AFR_ccRCC: Dropped 698435 non-autosomal rows.\n",
            "AFR_ccRCC: Dropped 0 rows with missing fields.\n",
            "AFR_ccRCC: Dropped 0 rows with invalid numeric values.\n",
            "AFR_ccRCC: Final clean shape: (18379162, 9)\n",
            "\n",
            "Saved clean files:\n",
            "  EUR_ccRCC.clean.autosomes.tsv\n",
            "  AFR_ccRCC.clean.autosomes.tsv\n",
            "\n",
            "Saved clump files:\n",
            "  EUR_ccRCC.for_clump.tsv\n",
            "  AFR_ccRCC.for_clump.tsv\n"
          ]
        }
      ]
    }
  ]
}